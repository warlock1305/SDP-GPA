# Project Organization & Script Documentation

## ğŸ“ Directory Structure

```
SDP GPA/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ collectors/           # Repository collection scripts
â”‚   â”œâ”€â”€ extraction/          # Feature extraction scripts
â”‚   â”œâ”€â”€ analysis/            # Analysis and processing scripts
â”‚   â””â”€â”€ models/              # Machine learning models
â”œâ”€â”€ data/                    # Data files (progress, metadata)
â”œâ”€â”€ dataset/                 # Collected repositories
â”œâ”€â”€ models/                  # Trained models
â”œâ”€â”€ config.yaml             # Configuration file
â””â”€â”€ PROJECT_SUMMARY.md      # Project overview
```

## ğŸ”§ Scripts Overview

### **ğŸ“¥ Collectors (`scripts/collectors/`)**

#### `corrected_single_contributor_collector.py` â­ **CURRENT WORKING VERSION**
- **Purpose**: Collects properly categorized single-contributor repositories
- **Features**: 
  - Skips already cloned repositories
  - Proper categorization (web_app, data_science, library, cli_tool, educational)
  - Debug logging
  - 100% success rate for verified repositories
- **Status**: âœ… **ACTIVE** - Used for current dataset collection
- **Output**: 29 repositories across 5 categories

#### `collector.py` ğŸ“š **ORIGINAL BASE**
- **Purpose**: Original repository collector with API integration
- **Features**: 
  - GitHub API integration
  - Token rotation
  - Rate limiting
  - File content extraction
- **Status**: ğŸ“– **REFERENCE** - Base implementation for other collectors

### **ğŸ” Extraction (`scripts/extraction/`)**

#### `extract_ast_features_for_analysis.py`
- **Purpose**: Extract AST features from collected repositories
- **Features**: 
  - Multi-language AST extraction
  - Feature vector generation
  - Path-based representations
- **Status**: ğŸ”„ **READY** - Next step in pipeline

#### `extract_codebert_embeddings.py`
- **Purpose**: Generate CodeBERT embeddings for code analysis
- **Features**: 
  - Multi-language code embeddings
  - Semantic representation
  - Batch processing
- **Status**: ğŸ”„ **READY** - Next step in pipeline

### **ğŸ“Š Analysis (`scripts/analysis/`)**

#### `enhanced_keyword_analyzer.py`
- **Purpose**: Keyword-based analysis of repositories
- **Features**: 
  - Pattern detection
  - Educational content identification
  - Quality metrics
- **Status**: ğŸ”„ **READY** - For comprehensive analysis

### **ğŸ¤– Models (`scripts/models/`)**

#### `comprehensive_random_forest_classifier.py`
- **Purpose**: Multi-modal Random Forest classifier
- **Features**: 
  - AST + CodeBERT + Keyword features
  - Architectural pattern classification
  - Multi-output prediction
- **Status**: ğŸ”„ **READY** - For training on collected dataset

## ğŸ—‘ï¸ Removed Redundant Scripts

The following scripts were **removed** as they were redundant or replaced by better versions:

### **âŒ Deleted Collectors:**
1. `single_contributor_collector.py` - Early attempt, replaced by `corrected_single_contributor_collector.py`
2. `public_repository_collector.py` - Alternative approach, not used
3. `enhanced_collector.py` - Enhancement of original, not used
4. `real_single_contributor_collector.py` - Replaced by corrected version
5. `small_single_contributor_collector.py` - Replaced by corrected version
6. `repository_collector.py` - Different approach, not used

### **ğŸ“Š Why They Were Removed:**
- **Duplication**: Multiple scripts doing the same thing
- **Better Alternatives**: `corrected_single_contributor_collector.py` is superior
- **Maintenance**: Fewer files to maintain
- **Confusion**: Clear which script to use

## ğŸ“ˆ Current Status

### **âœ… Completed:**
- Repository collection (29 repos across 5 categories)
- Dataset organization
- Script cleanup and organization
- CLI tools population

### **ğŸ”„ Next Steps:**
1. **AST Feature Extraction**: Run `extract_ast_features_for_analysis.py`
2. **CodeBERT Embeddings**: Run `extract_codebert_embeddings.py`
3. **Keyword Analysis**: Run `enhanced_keyword_analyzer.py`
4. **Model Training**: Run `comprehensive_random_forest_classifier.py`

## ğŸ¯ Usage Instructions

### **To Collect More Repositories:**
```bash
python scripts/collectors/corrected_single_contributor_collector.py
```

### **To Extract AST Features:**
```bash
python scripts/extraction/extract_ast_features_for_analysis.py
```

### **To Generate CodeBERT Embeddings:**
```bash
python scripts/extraction/extract_codebert_embeddings.py
```

### **To Train the Model:**
```bash
python scripts/models/comprehensive_random_forest_classifier.py
```

## ğŸ“‹ Data Files

### **Progress Files (`data/`):**
- `corrected_single_contributor_progress.json` - Current collection progress
- `corrected_single_contributor_metadata.csv` - Repository metadata

### **Dataset (`dataset/`):**
- `web_application/` - 10 repositories
- `data_science/` - 3 repositories  
- `library/` - 7 repositories
- `cli_tool/` - 7 repositories
- `educational/` - 13 repositories

## ğŸ”§ Configuration

### **`config.yaml`:**
- Supported languages
- GitHub API configuration
- File extensions
- Collection limits

## ğŸ“š Documentation

### **`PROJECT_SUMMARY.md`:**
- Complete project overview
- Technical details
- Implementation status
- Next steps

---

**ğŸ‰ Result**: Clean, organized project structure with clear separation of concerns and no redundant scripts!
